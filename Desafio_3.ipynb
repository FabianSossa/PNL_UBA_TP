{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOTR33zKGr1K6eZ2GBCzx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabianSossa/PNL_UBA_TP/blob/main/Desafio_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres"
      ],
      "metadata": {
        "id": "r-1KwO-Qa6Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ],
      "metadata": {
        "id": "TCOAj68la9jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "58GY5E5EbR9_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datos"
      ],
      "metadata": {
        "id": "z8jbwMHXbt9q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "viRBxoFpWu9E"
      },
      "outputs": [],
      "source": [
        "# descargar de textos.info\n",
        "import urllib.request\n",
        "\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_html = urllib.request.urlopen('https://www.textos.info/julio-verne/la-vuelta-al-mundo-en-80-dias/ebook')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Parsear artículo, 'lxml' es el parser a utilizar\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Encontrar todos los párrafos del HTML (bajo el tag <p>)\n",
        "# y tenerlos disponible como lista\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text + ' '\n",
        "\n",
        "# pasar todo el texto a minúscula\n",
        "article_text = article_text.lower()"
      ],
      "metadata": {
        "id": "ewOrtYmcbwYD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en article text se encuentra el texto de todo el libro\n",
        "article_text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "Z88k3XVybycp",
        "outputId": "7e59fe9d-75c4-456a-eb35-1a8d5fdbd560"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' en el año 1872, la casa número 7 de saville-row, burlington gardens \\r\\n—donde murió sheridan en 1814— estaba habitada por phileas fogg, quien a\\r\\n pesar de que parecía haber tomado el partido de no hacer nada que \\r\\npudiese llamar la atención, era uno de los miembros más notables y \\r\\nsingulares del reformclub de londres. por consiguiente, phileas fogg, personaje enigmático y del cual sólo \\r\\nse sabía que era un hombre muy galante y de los más cumplidos gentlemen \\r\\nde la alta sociedad inglesa, sucedía a uno de los más grandes oradores \\r\\nque honran a inglaterra. decíase que se daba un aire a lo byron —su cabeza, se entiende, \\r\\nporque, en cuanto a los pies, no tenía defecto alguno—, pero a un byron \\r\\nde bigote y pastillas, a un byron impasible, que hubiera vivido mil años\\r\\n sin envejecer. phileas fogg, era inglés de pura cepa; pero quizás no había nacido en\\r\\n londres. jamás se le había visto en la bolsa ni en el banco, ni en \\r\\nninguno de los despachos mercantiles de la city. ni las dársenas '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elegir el tamaño del contexto\n",
        "\n",
        "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
        "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
        "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
      ],
      "metadata": {
        "id": "-_WNFF45cjUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seleccionamos el tamaño de contexto\n",
        "max_context_size = 100"
      ],
      "metadata": {
        "id": "zqaLXgj3b16a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ],
      "metadata": {
        "id": "bsl3g2Bvcl-M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en este caso el vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
        "chars_vocab = set(article_text)"
      ],
      "metadata": {
        "id": "-qEyDj3Ocnlx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# la longitud de vocabulario de caracteres es:\n",
        "len(chars_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqFFz8KCcpBG",
        "outputId": "5f100335-ac91-42dd-970d-2dde4a9ff739"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ],
      "metadata": {
        "id": "65mWM4wrcqqp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Tokenizar"
      ],
      "metadata": {
        "id": "fQIj3oKccuWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ],
      "metadata": {
        "id": "UgXvH1wmcsho"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgMJEiH3cwiZ",
        "outputId": "44c65695-a6bf-4404-b578-c17919ee5c24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 48,\n",
              " 44,\n",
              " 5,\n",
              " 20,\n",
              " 19,\n",
              " 21,\n",
              " 5,\n",
              " 0,\n",
              " 17,\n",
              " 41,\n",
              " 55,\n",
              " 31,\n",
              " 5,\n",
              " 44,\n",
              " 20,\n",
              " 5,\n",
              " 63,\n",
              " 20,\n",
              " 25,\n",
              " 20,\n",
              " 5,\n",
              " 7,\n",
              " 61,\n",
              " 6,\n",
              " 48,\n",
              " 18,\n",
              " 21,\n",
              " 5,\n",
              " 41,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 25,\n",
              " 20,\n",
              " 37,\n",
              " 57,\n",
              " 44,\n",
              " 44,\n",
              " 48,\n",
              " 11,\n",
              " 18,\n",
              " 21,\n",
              " 3,\n",
              " 31,\n",
              " 5,\n",
              " 27,\n",
              " 33,\n",
              " 18,\n",
              " 44,\n",
              " 57,\n",
              " 7,\n",
              " 46,\n",
              " 47,\n",
              " 21,\n",
              " 7,\n",
              " 5,\n",
              " 46,\n",
              " 20,\n",
              " 18,\n",
              " 14,\n",
              " 48,\n",
              " 7,\n",
              " 25,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 45,\n",
              " 14,\n",
              " 21,\n",
              " 7,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 6,\n",
              " 33,\n",
              " 18,\n",
              " 57,\n",
              " 66,\n",
              " 5,\n",
              " 25,\n",
              " 56,\n",
              " 48,\n",
              " 18,\n",
              " 57,\n",
              " 14,\n",
              " 20,\n",
              " 7,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 0,\n",
              " 17,\n",
              " 0,\n",
              " 15,\n",
              " 45,\n",
              " 5,\n",
              " 48,\n",
              " 25,\n",
              " 47,\n",
              " 20,\n",
              " 27,\n",
              " 20,\n",
              " 5,\n",
              " 56,\n",
              " 20,\n",
              " 27,\n",
              " 57,\n",
              " 47,\n",
              " 20,\n",
              " 14,\n",
              " 20,\n",
              " 5,\n",
              " 8,\n",
              " 21,\n",
              " 18,\n",
              " 5,\n",
              " 8,\n",
              " 56,\n",
              " 57,\n",
              " 44,\n",
              " 48,\n",
              " 20,\n",
              " 25,\n",
              " 5,\n",
              " 40,\n",
              " 21,\n",
              " 46,\n",
              " 46,\n",
              " 31,\n",
              " 5,\n",
              " 62,\n",
              " 33,\n",
              " 57,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 20,\n",
              " 50,\n",
              " 64,\n",
              " 5,\n",
              " 8,\n",
              " 48,\n",
              " 25,\n",
              " 20,\n",
              " 18,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 62,\n",
              " 33,\n",
              " 48,\n",
              " 5,\n",
              " 8,\n",
              " 20,\n",
              " 18,\n",
              " 48,\n",
              " 63,\n",
              " 53,\n",
              " 20,\n",
              " 5,\n",
              " 56,\n",
              " 20,\n",
              " 27,\n",
              " 48,\n",
              " 18,\n",
              " 5,\n",
              " 47,\n",
              " 21,\n",
              " 6,\n",
              " 20,\n",
              " 14,\n",
              " 21,\n",
              " 5,\n",
              " 48,\n",
              " 44,\n",
              " 5,\n",
              " 8,\n",
              " 20,\n",
              " 18,\n",
              " 47,\n",
              " 57,\n",
              " 14,\n",
              " 21,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 7,\n",
              " 21,\n",
              " 5,\n",
              " 56,\n",
              " 20,\n",
              " 63,\n",
              " 48,\n",
              " 18,\n",
              " 5,\n",
              " 7,\n",
              " 20,\n",
              " 14,\n",
              " 20,\n",
              " 5,\n",
              " 62,\n",
              " 33,\n",
              " 48,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 8,\n",
              " 33,\n",
              " 14,\n",
              " 57,\n",
              " 48,\n",
              " 25,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 44,\n",
              " 20,\n",
              " 6,\n",
              " 20,\n",
              " 18,\n",
              " 5,\n",
              " 44,\n",
              " 20,\n",
              " 5,\n",
              " 20,\n",
              " 47,\n",
              " 48,\n",
              " 7,\n",
              " 63,\n",
              " 57,\n",
              " 66,\n",
              " 7,\n",
              " 31,\n",
              " 5,\n",
              " 48,\n",
              " 18,\n",
              " 20,\n",
              " 5,\n",
              " 33,\n",
              " 7,\n",
              " 21,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 6,\n",
              " 57,\n",
              " 48,\n",
              " 6,\n",
              " 27,\n",
              " 18,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 6,\n",
              " 49,\n",
              " 25,\n",
              " 5,\n",
              " 7,\n",
              " 21,\n",
              " 47,\n",
              " 20,\n",
              " 27,\n",
              " 44,\n",
              " 48,\n",
              " 25,\n",
              " 5,\n",
              " 26,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 25,\n",
              " 57,\n",
              " 7,\n",
              " 46,\n",
              " 33,\n",
              " 44,\n",
              " 20,\n",
              " 18,\n",
              " 48,\n",
              " 25,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 44,\n",
              " 5,\n",
              " 18,\n",
              " 48,\n",
              " 40,\n",
              " 21,\n",
              " 18,\n",
              " 6,\n",
              " 63,\n",
              " 44,\n",
              " 33,\n",
              " 27,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 7,\n",
              " 14,\n",
              " 18,\n",
              " 48,\n",
              " 25,\n",
              " 22,\n",
              " 5,\n",
              " 8,\n",
              " 21,\n",
              " 18,\n",
              " 5,\n",
              " 63,\n",
              " 21,\n",
              " 7,\n",
              " 25,\n",
              " 57,\n",
              " 46,\n",
              " 33,\n",
              " 57,\n",
              " 48,\n",
              " 7,\n",
              " 47,\n",
              " 48,\n",
              " 31,\n",
              " 5,\n",
              " 8,\n",
              " 56,\n",
              " 57,\n",
              " 44,\n",
              " 48,\n",
              " 20,\n",
              " 25,\n",
              " 5,\n",
              " 40,\n",
              " 21,\n",
              " 46,\n",
              " 46,\n",
              " 31,\n",
              " 5,\n",
              " 8,\n",
              " 48,\n",
              " 18,\n",
              " 25,\n",
              " 21,\n",
              " 7,\n",
              " 20,\n",
              " 4,\n",
              " 48,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 57,\n",
              " 46,\n",
              " 6,\n",
              " 49,\n",
              " 47,\n",
              " 57,\n",
              " 63,\n",
              " 21,\n",
              " 5,\n",
              " 26,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 44,\n",
              " 5,\n",
              " 63,\n",
              " 33,\n",
              " 20,\n",
              " 44,\n",
              " 5,\n",
              " 25,\n",
              " 66,\n",
              " 44,\n",
              " 21,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 25,\n",
              " 48,\n",
              " 5,\n",
              " 25,\n",
              " 20,\n",
              " 27,\n",
              " 53,\n",
              " 20,\n",
              " 5,\n",
              " 62,\n",
              " 33,\n",
              " 48,\n",
              " 5,\n",
              " 48,\n",
              " 18,\n",
              " 20,\n",
              " 5,\n",
              " 33,\n",
              " 7,\n",
              " 5,\n",
              " 56,\n",
              " 21,\n",
              " 6,\n",
              " 27,\n",
              " 18,\n",
              " 48,\n",
              " 5,\n",
              " 6,\n",
              " 33,\n",
              " 26,\n",
              " 5,\n",
              " 46,\n",
              " 20,\n",
              " 44,\n",
              " 20,\n",
              " 7,\n",
              " 47,\n",
              " 48,\n",
              " 5,\n",
              " 26,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 6,\n",
              " 49,\n",
              " 25,\n",
              " 5,\n",
              " 63,\n",
              " 33,\n",
              " 6,\n",
              " 8,\n",
              " 44,\n",
              " 57,\n",
              " 14,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 46,\n",
              " 48,\n",
              " 7,\n",
              " 47,\n",
              " 44,\n",
              " 48,\n",
              " 6,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 20,\n",
              " 5,\n",
              " 20,\n",
              " 44,\n",
              " 47,\n",
              " 20,\n",
              " 5,\n",
              " 25,\n",
              " 21,\n",
              " 63,\n",
              " 57,\n",
              " 48,\n",
              " 14,\n",
              " 20,\n",
              " 14,\n",
              " 5,\n",
              " 57,\n",
              " 7,\n",
              " 46,\n",
              " 44,\n",
              " 48,\n",
              " 25,\n",
              " 20,\n",
              " 31,\n",
              " 5,\n",
              " 25,\n",
              " 33,\n",
              " 63,\n",
              " 48,\n",
              " 14,\n",
              " 53,\n",
              " 20,\n",
              " 5,\n",
              " 20,\n",
              " 5,\n",
              " 33,\n",
              " 7,\n",
              " 21,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 6,\n",
              " 49,\n",
              " 25,\n",
              " 5,\n",
              " 46,\n",
              " 18,\n",
              " 20,\n",
              " 7,\n",
              " 14,\n",
              " 48,\n",
              " 25,\n",
              " 5,\n",
              " 21,\n",
              " 18,\n",
              " 20,\n",
              " 14,\n",
              " 21,\n",
              " 18,\n",
              " 48,\n",
              " 25,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 62,\n",
              " 33,\n",
              " 48,\n",
              " 5,\n",
              " 56,\n",
              " 21,\n",
              " 7,\n",
              " 18,\n",
              " 20,\n",
              " 7,\n",
              " 5,\n",
              " 20,\n",
              " 5,\n",
              " 57,\n",
              " 7,\n",
              " 46,\n",
              " 44,\n",
              " 20,\n",
              " 47,\n",
              " 48,\n",
              " 18,\n",
              " 18,\n",
              " 20,\n",
              " 22,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 63,\n",
              " 53,\n",
              " 20,\n",
              " 25,\n",
              " 48,\n",
              " 5,\n",
              " 62,\n",
              " 33,\n",
              " 48,\n",
              " 5,\n",
              " 25,\n",
              " 48,\n",
              " 5,\n",
              " 14,\n",
              " 20,\n",
              " 27,\n",
              " 20,\n",
              " 5,\n",
              " 33,\n",
              " 7,\n",
              " 5,\n",
              " 20,\n",
              " 57,\n",
              " 18,\n",
              " 48,\n",
              " 5,\n",
              " 20,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 5,\n",
              " 27,\n",
              " 26,\n",
              " 18,\n",
              " 21,\n",
              " 7,\n",
              " 5,\n",
              " 45,\n",
              " 25,\n",
              " 33,\n",
              " 5,\n",
              " 63,\n",
              " 20,\n",
              " 27,\n",
              " 48,\n",
              " 12,\n",
              " 20,\n",
              " 31,\n",
              " 5,\n",
              " 25,\n",
              " 48,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 47,\n",
              " 57,\n",
              " 48,\n",
              " 7,\n",
              " 14,\n",
              " 48,\n",
              " 31,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 8,\n",
              " 21,\n",
              " 18,\n",
              " 62,\n",
              " 33,\n",
              " 48,\n",
              " 31,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 63,\n",
              " 33,\n",
              " 20,\n",
              " 7,\n",
              " 47,\n",
              " 21,\n",
              " 5,\n",
              " 20,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 8,\n",
              " 57,\n",
              " 48,\n",
              " 25,\n",
              " 31,\n",
              " 5,\n",
              " 7,\n",
              " 21,\n",
              " 5,\n",
              " 47,\n",
              " 48,\n",
              " 7,\n",
              " 53,\n",
              " 20,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 40,\n",
              " 48,\n",
              " 63,\n",
              " 47,\n",
              " 21,\n",
              " 5,\n",
              " 20,\n",
              " 44,\n",
              " 46,\n",
              " 33,\n",
              " 7,\n",
              " 21,\n",
              " 45,\n",
              " 31,\n",
              " 5,\n",
              " 8,\n",
              " 48,\n",
              " 18,\n",
              " 21,\n",
              " 5,\n",
              " 20,\n",
              " 5,\n",
              " 33,\n",
              " 7,\n",
              " 5,\n",
              " 27,\n",
              " 26,\n",
              " 18,\n",
              " 21,\n",
              " 7,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 27,\n",
              " 57,\n",
              " 46,\n",
              " 21,\n",
              " 47,\n",
              " 48,\n",
              " 5,\n",
              " 26,\n",
              " 5,\n",
              " 8,\n",
              " 20,\n",
              " 25,\n",
              " 47,\n",
              " 57,\n",
              " 44,\n",
              " 44,\n",
              " 20,\n",
              " 25,\n",
              " 31,\n",
              " 5,\n",
              " 20,\n",
              " 5,\n",
              " 33,\n",
              " 7,\n",
              " 5,\n",
              " 27,\n",
              " 26,\n",
              " 18,\n",
              " 21,\n",
              " 7,\n",
              " 5,\n",
              " 57,\n",
              " 6,\n",
              " 8,\n",
              " 20,\n",
              " 25,\n",
              " 57,\n",
              " 27,\n",
              " 44,\n",
              " 48,\n",
              " 31,\n",
              " 5,\n",
              " 62,\n",
              " 33,\n",
              " 48,\n",
              " 5,\n",
              " 56,\n",
              " 33,\n",
              " 27,\n",
              " 57,\n",
              " 48,\n",
              " 18,\n",
              " 20,\n",
              " 5,\n",
              " 37,\n",
              " 57,\n",
              " 37,\n",
              " 57,\n",
              " 14,\n",
              " 21,\n",
              " 5,\n",
              " 6,\n",
              " 57,\n",
              " 44,\n",
              " 5,\n",
              " 20,\n",
              " 19,\n",
              " 21,\n",
              " 25,\n",
              " 50,\n",
              " 64,\n",
              " 5,\n",
              " 25,\n",
              " 57,\n",
              " 7,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 37,\n",
              " 48,\n",
              " 4,\n",
              " 48,\n",
              " 63,\n",
              " 48,\n",
              " 18,\n",
              " 22,\n",
              " 5,\n",
              " 8,\n",
              " 56,\n",
              " 57,\n",
              " 44,\n",
              " 48,\n",
              " 20,\n",
              " 25,\n",
              " 5,\n",
              " 40,\n",
              " 21,\n",
              " 46,\n",
              " 46,\n",
              " 31,\n",
              " 5,\n",
              " 48,\n",
              " 18,\n",
              " 20,\n",
              " 5,\n",
              " 57,\n",
              " 7,\n",
              " 46,\n",
              " 44,\n",
              " 58,\n",
              " 25,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 8,\n",
              " 33,\n",
              " 18,\n",
              " 20,\n",
              " 5,\n",
              " 63,\n",
              " 48,\n",
              " 8,\n",
              " 20,\n",
              " 42,\n",
              " 5,\n",
              " 8,\n",
              " 48,\n",
              " 18,\n",
              " 21,\n",
              " 5,\n",
              " 62,\n",
              " 33,\n",
              " 57,\n",
              " 12,\n",
              " 49,\n",
              " 25,\n",
              " 5,\n",
              " 7,\n",
              " 21,\n",
              " 5,\n",
              " 56,\n",
              " 20,\n",
              " 27,\n",
              " 53,\n",
              " 20,\n",
              " 5,\n",
              " 7,\n",
              " 20,\n",
              " 63,\n",
              " 57,\n",
              " 14,\n",
              " 21,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 50,\n",
              " 64,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 7,\n",
              " 14,\n",
              " 18,\n",
              " 48,\n",
              " 25,\n",
              " 22,\n",
              " 5,\n",
              " 4,\n",
              " 20,\n",
              " 6,\n",
              " 49,\n",
              " 25,\n",
              " 5,\n",
              " 25,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 48,\n",
              " 5,\n",
              " 56,\n",
              " 20,\n",
              " 27,\n",
              " 53,\n",
              " 20,\n",
              " 5,\n",
              " 37,\n",
              " 57,\n",
              " 25,\n",
              " 47,\n",
              " 21,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 44,\n",
              " 20,\n",
              " 5,\n",
              " 27,\n",
              " 21,\n",
              " 44,\n",
              " 25,\n",
              " 20,\n",
              " 5,\n",
              " 7,\n",
              " 57,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 48,\n",
              " 44,\n",
              " 5,\n",
              " 27,\n",
              " 20,\n",
              " 7,\n",
              " 63,\n",
              " 21,\n",
              " 31,\n",
              " 5,\n",
              " 7,\n",
              " 57,\n",
              " 5,\n",
              " 48,\n",
              " 7,\n",
              " 5,\n",
              " 50,\n",
              " 64,\n",
              " 7,\n",
              " 57,\n",
              " 7,\n",
              " 46,\n",
              " 33,\n",
              " 7,\n",
              " 21,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 25,\n",
              " 8,\n",
              " 20,\n",
              " 63,\n",
              " 56,\n",
              " 21,\n",
              " 25,\n",
              " 5,\n",
              " 6,\n",
              " 48,\n",
              " 18,\n",
              " 63,\n",
              " 20,\n",
              " 7,\n",
              " 47,\n",
              " 57,\n",
              " 44,\n",
              " 48,\n",
              " 25,\n",
              " 5,\n",
              " 14,\n",
              " 48,\n",
              " 5,\n",
              " 44,\n",
              " 20,\n",
              " 5,\n",
              " 63,\n",
              " 57,\n",
              " 47,\n",
              " 26,\n",
              " 22,\n",
              " 5,\n",
              " 7,\n",
              " 57,\n",
              " 5,\n",
              " 44,\n",
              " 20,\n",
              " 25,\n",
              " 5,\n",
              " 14,\n",
              " 49,\n",
              " 18,\n",
              " 25,\n",
              " 48,\n",
              " 7,\n",
              " 20,\n",
              " 25,\n",
              " 5]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Organizando y estructurando el dataset"
      ],
      "metadata": {
        "id": "HrxX3MO9c0th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separaremos el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
      ],
      "metadata": {
        "id": "0imb35PRcxzM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]"
      ],
      "metadata": {
        "id": "5l3nl7bJc2mD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
      ],
      "metadata": {
        "id": "8AFg13LZc4Ix"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
      ],
      "metadata": {
        "id": "bxIV224Bc6je"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ],
      "metadata": {
        "id": "-SVV1oLfc8jw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
        "\n",
        "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
        "\n",
        "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
      ],
      "metadata": {
        "id": "NG3WoocTdCRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ],
      "metadata": {
        "id": "pUnFtTJydFCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw4OgIAsc_xF",
        "outputId": "b3987d56-355b-4a12-d3d8-d42a775ebdd4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(359671, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0,:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0tneTKbdHGi",
        "outputId": "434d578e-4e16-476f-feec-d6051e06dd4e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5, 48,  7,  5, 48, 44,  5, 20, 19, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0,:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WkRv1gDdIjx",
        "outputId": "0387ba38-597f-4714-c136-72274aa59829"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([48,  7,  5, 48, 44,  5, 20, 19, 21,  5])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars_vocab)"
      ],
      "metadata": {
        "id": "0ZJFYupjdKZG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definiendo el modelo"
      ],
      "metadata": {
        "id": "X1gXHBm4dOWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
        "from keras.models import Model, Sequential"
      ],
      "metadata": {
        "id": "4qsFGLBidMm0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ],
      "metadata": {
        "id": "EjhJz6s5dTYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
        "model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "jqTKmTwudRD9",
        "outputId": "7d7ba363-f7f6-482c-8b6a-9ef70db4b201"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │        \u001b[38;5;34m53,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)       │        \u001b[38;5;34m13,668\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,668</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,468\u001b[0m (263.55 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,468</span> (263.55 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,468\u001b[0m (263.55 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,468</span> (263.55 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Definir el modelo"
      ],
      "metadata": {
        "id": "0AgDNwb1dYGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que por el momento no hay implementaciones adecuadas de la perplejidad que puedan operar en tiempo de entrenamiento, armaremos un Callback *ad-hoc* que la calcule en cada epoch.\n",
        "\n",
        "**Nota**: un Callback es una rutina gatillada por algún evento, son muy útiles para relevar datos en diferentes momentos del desarrollo del modelo. En este caso queremos hacer un cálculo cada vez que termina una epoch de entrenamiento."
      ],
      "metadata": {
        "id": "ZTcnsJ2YdZ_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl,patience=5):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "        if len(subseq)!=0:\n",
        "\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(\"my_model_LSTM_GRU.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True"
      ],
      "metadata": {
        "id": "I0RyxPexdVJb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "w_slG9RddfZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
        "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
        "history_ppl = []\n",
        "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val,history_ppl)], batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MKTG6fPddud",
        "outputId": "1f7907a3-9112-4859-f9ef-ccda4f07a046"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.4432\n",
            " mean perplexity: 6.326935208622535 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 35ms/step - loss: 2.4431\n",
            "Epoch 2/20\n",
            "\u001b[1m1404/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.9295\n",
            " mean perplexity: 5.515080337548376 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 30ms/step - loss: 1.9294\n",
            "Epoch 3/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8240\n",
            " mean perplexity: 5.378858212849603 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 31ms/step - loss: 1.8240\n",
            "Epoch 4/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7774\n",
            " mean perplexity: 5.264752984046936 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 25ms/step - loss: 1.7774\n",
            "Epoch 5/20\n",
            "\u001b[1m1404/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7493\n",
            " mean perplexity: 5.301826719063611 \n",
            "\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 30ms/step - loss: 1.7493\n",
            "Epoch 6/20\n",
            "\u001b[1m1404/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7323\n",
            " mean perplexity: 5.151337260576947 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.7323\n",
            "Epoch 7/20\n",
            "\u001b[1m1404/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7189\n",
            " mean perplexity: 4.95799591493367 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 25ms/step - loss: 1.7188\n",
            "Epoch 8/20\n",
            "\u001b[1m1402/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7087\n",
            " mean perplexity: 4.872822505744858 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 25ms/step - loss: 1.7087\n",
            "Epoch 9/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6998\n",
            " mean perplexity: 4.829041747591603 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 25ms/step - loss: 1.6997\n",
            "Epoch 10/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6937\n",
            " mean perplexity: 4.798246403435367 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 25ms/step - loss: 1.6937\n",
            "Epoch 11/20\n",
            "\u001b[1m1404/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6877\n",
            " mean perplexity: 4.728608267990189 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 30ms/step - loss: 1.6877\n",
            "Epoch 12/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6815\n",
            " mean perplexity: 4.70125131511209 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 25ms/step - loss: 1.6815\n",
            "Epoch 13/20\n",
            "\u001b[1m1404/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6776\n",
            " mean perplexity: 4.7096723923132044 \n",
            "\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - loss: 1.6776\n",
            "Epoch 14/20\n",
            "\u001b[1m1402/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6747\n",
            " mean perplexity: 4.685364935865354 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 31ms/step - loss: 1.6747\n",
            "Epoch 15/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6708\n",
            " mean perplexity: 4.678664772354778 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 25ms/step - loss: 1.6708\n",
            "Epoch 16/20\n",
            "\u001b[1m1404/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6678\n",
            " mean perplexity: 4.6560858530015805 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 31ms/step - loss: 1.6678\n",
            "Epoch 17/20\n",
            "\u001b[1m1403/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6661\n",
            " mean perplexity: 4.672556681848651 \n",
            "\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 25ms/step - loss: 1.6661\n",
            "Epoch 18/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6628\n",
            " mean perplexity: 4.6349939736888635 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - loss: 1.6628\n",
            "Epoch 19/20\n",
            "\u001b[1m1403/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6603\n",
            " mean perplexity: 4.644456804098196 \n",
            "\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - loss: 1.6603\n",
            "Epoch 20/20\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6585\n",
            " mean perplexity: 4.6236640628258785 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m1405/1405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30ms/step - loss: 1.6585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "EV_zBQpXdhzB",
        "outputId": "5a6ca510-ec74-4bd3-e8be-5b604df761ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el mejor modelo guardado del entrenamiento para hacer inferencia\n",
        "model = keras.models.load_model('my_model_LSTM_GRU.keras')"
      ],
      "metadata": {
        "id": "plWQhEFciunS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicción del próximo caracter"
      ],
      "metadata": {
        "id": "WMFTt2QWjDfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se puede usar gradio para probar el modelo\n",
        "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
        "# https://gradio.app/\n",
        "\n",
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "zUVKDbibizqI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
        "\n",
        "\n",
        "    # Debemos buscar en el vocabulario el caracter\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    out_word = idx2char[y_hat]\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "pWCfxxDXjFod",
        "outputId": "5722f445-57b8-4e40-85cf-852daa525a4b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://99a7ed8e0e211475d8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://99a7ed8e0e211475d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://99a7ed8e0e211475d8.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Do6_ABGejKMa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}